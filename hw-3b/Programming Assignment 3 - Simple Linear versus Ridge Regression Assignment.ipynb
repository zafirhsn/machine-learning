{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 3 - Simple Linear versus Ridge Regression (70 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your friend Bob just moved to Boston. He is a real estate agent who is trying to evaluate the prices of houses in the Boston area. He has been using a linear regression model but it has been taking a long time to compute prices for new hourse. He comes to you for help as he knows that you're an expert in machine learning. As a pro, you suggest ridge regression to reduce the feature weights as a way to increase the speed of computation. Bob, however, being a skeptic isn't convinced. He wants you to write a program that illustrates the difference in training and test costs for both linear and ridge regression on the same dataset. Being a good friend, you oblige and hence this assignment :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
      " ...\n",
      " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
      " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
      " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n"
     ]
    }
   ],
   "source": [
    "# TODO - Import the boston dataset from sklearn - 5 points. \n",
    "boston_data = load_boston();\n",
    "print(boston_data.data.shape)\n",
    "print(boston_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Create X and Y variables - X holding the .data and Y holding .target - 5 points\n",
    "X = boston_data.data\n",
    "Y = boston_data.target\n",
    "\n",
    "\n",
    "# TODO - Reshape Y to be a rank 2 matrix - 5 points\n",
    "Y = Y.reshape(Y.shape[0], 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scaling and centering of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create polynomial feeatures for the dataset to test linear and ridge regression on data with d = 1 and data with d = 2. Feel free to increase the # of degress and see what effect it has on the training and test error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Create a PolynomialFeatures object with degree = 2. \n",
    "# Transform X and save it into X_2. Simply copy Y into Y_2 - 10 points\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_2 = poly.fit_transform(X)\n",
    "Y_2 = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 105)\n",
      "(506, 1)\n"
     ]
    }
   ],
   "source": [
    "# CHECK - the shape of X_2 and Y_2 - should be (506, 105) and (506, 1) respectively\n",
    "print(X_2.shape)\n",
    "print(Y_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split both the sets of data into training and testing data to get a more accurate picture of real-world performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 13) (152, 13) (354, 1) (152, 1)\n",
      "(354, 105) (152, 105) (354, 1) (152, 1)\n"
     ]
    }
   ],
   "source": [
    "# TODO - Call train_test_split for both X, Y and X_2, Y_2. Don't pass in any other parameters except for the dataset. \n",
    "# Let the split be the default value of 0.7 => 70% training and 30% test. - 10 points\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.7, test_size=0.3)\n",
    "X_2_train, X_2_test, Y_2_train, Y_2_test = train_test_split(X_2, Y_2, train_size=0.7, test_size=0.3)\n",
    "\n",
    "# CHECK - Shapes of the resulting variables should be as follows:\n",
    "# (379, 13) (127, 13) (379, 1) (127, 1)\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "# (379, 105) (127, 105) (379, 1) (127, 1)\n",
    "print(X_2_train.shape, X_2_test.shape, Y_2_train.shape, Y_2_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression and Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Define the get_error_ridge function. Use Ridge() from sklearn.\n",
    "# TODO - Return train_error and test_error values\n",
    "# Some important functions to know are .fit() and .predict()\n",
    "# HINT - fit() the data on training data and predict() on training and testing data to get the train and test error\n",
    "# 15 points\n",
    "\n",
    "def get_error_ridge(X_train, X_test, Y_train, Y_test, alpha):\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    ridge = sklearn.linear_model.Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, Y_train)\n",
    "    \n",
    "    y_hat_train = ridge.predict(X_train)\n",
    "    y_hat_test = ridge.predict(X_test)\n",
    "    train_error = mean_squared_error(Y_train, y_hat_train)\n",
    "    test_error = mean_squared_error(Y_test, y_hat_test)\n",
    "    \n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For linear regression and no polynomial features: train_error = 22.30 and test_error = 21.74\n",
      "For linear regression and with polynomial features: train_error = 4.53 and test_error = 13.91\n"
     ]
    }
   ],
   "source": [
    "# Call the function above with alpha = 0 - Notice that when alpha = 0, ridge regression and linear regression are equivalent\n",
    "lin_rss = get_error_ridge(X_train, X_test, Y_train, Y_test, 0)\n",
    "lin_poly_rss = get_error_ridge(X_2_train, X_2_test, Y_2_train, Y_2_test, 0)\n",
    "\n",
    "get_error_ridge(X_train, X_test, Y_train, Y_test, 0)\n",
    "print(\"For linear regression and no polynomial features: train_error = {0:.2f} and test_error = {1:.2f}\".format(lin_rss[0], lin_rss[1]))\n",
    "print(\"For linear regression and with polynomial features: train_error = {0:.2f} and test_error = {1:.2f}\".format(lin_poly_rss[0], lin_poly_rss[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ridge regression and no polynomial features: train_error = 22.48 and test_error = 21.65\n",
      "For ridge regression and with polynomial features: train_error = 4.97 and test_error = 13.52\n"
     ]
    }
   ],
   "source": [
    "# Call the function above with alpha = 1\n",
    "lin_rss = get_error_ridge(X_train, X_test, Y_train, Y_test, 0.75)\n",
    "lin_poly_rss = get_error_ridge(X_2_train, X_2_test, Y_2_train, Y_2_test, 0.75)\n",
    "\n",
    "print(\"For ridge regression and no polynomial features: train_error = {0:.2f} and test_error = {1:.2f}\".format(lin_rss[0], lin_rss[1]))\n",
    "print(\"For ridge regression and with polynomial features: train_error = {0:.2f} and test_error = {1:.2f}\".format(lin_poly_rss[0], lin_poly_rss[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Define the get_coeff_ridge_normaleq function. Use the normal equation method.\n",
    "# TODO - Return w values\n",
    "# 10 points\n",
    "\n",
    "def get_coeff_ridge_normaleq(X_train, Y_train, alpha):\n",
    "    # a = X.T.(X) + alpha * np.identity(# of columns in X)\n",
    "    a = None\n",
    "    # q is the pseudo inverse of a\n",
    "    q = None\n",
    "    # z is X.T.dot(Y)\n",
    "    z = None\n",
    "    # w = q dot z - reshape to (# columns in X, 1)\n",
    "    w = None\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Define the evaluate_err_ridge_normaleq function.\n",
    "# TODO - Return the train_error and test_error values\n",
    "# 10 points\n",
    "\n",
    "def evaluate_err_ridge_normaleq(X_train, X_test, Y_train, Y_test, w): \n",
    "    train_error = None\n",
    "    test_error = None\n",
    "    \n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For linear regression and no polynomial features: train_error = 23.63 and test_error = 27.07\n",
      "For linear regression and with polynomial features: train_error = 4.87 and test_error = 16.85\n"
     ]
    }
   ],
   "source": [
    "# Call the function above with alpha = 0 - Notice that when alpha = 0, ridge regression and linear regression are equivalent\n",
    "w = get_coeff_ridge_normaleq(X_train, Y_train, 0)\n",
    "lin_rss = evaluate_err_ridge_normaleq(X_train, X_test, Y_train, Y_test, w)\n",
    "\n",
    "w_2 = get_coeff_ridge_normaleq(X_2_train, Y_2_train, 0)\n",
    "lin_poly_rss = evaluate_err_ridge_normaleq(X_2_train, X_2_test, Y_2_train, Y_2_test, w_2)\n",
    "\n",
    "print(\"For linear regression and no polynomial features: train_error = {0:.2f} and test_error = {1:.2f}\".format(lin_rss[0], lin_rss[1]))\n",
    "print(\"For linear regression and with polynomial features: train_error = {0:.2f} and test_error = {1:.2f}\".format(lin_poly_rss[0], lin_poly_rss[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ridge regression and no polynomial features: train_error = 23.64 and test_error = 26.81\n",
      "For ridge regression and with polynomial features: train_error = 5.10 and test_error = 18.14\n"
     ]
    }
   ],
   "source": [
    "# Call the function above with alpha = 1\n",
    "w = get_coeff_ridge_normaleq(X_train, Y_train, 0.75)\n",
    "lin_rss = evaluate_err_ridge_normaleq(X_train, X_test, Y_train, Y_test, w)\n",
    "\n",
    "w_2 = get_coeff_ridge_normaleq(X_2_train, Y_2_train, 0.75)\n",
    "lin_poly_rss = evaluate_err_ridge_normaleq(X_2_train, X_2_test, Y_2_train, Y_2_test, w_2)\n",
    "\n",
    "print(\"For ridge regression and no polynomial features: train_error = {0:.2f} and test_error = {1:.2f}\".format(lin_rss[0], lin_rss[1]))\n",
    "print(\"For ridge regression and with polynomial features: train_error = {0:.2f} and test_error = {1:.2f}\".format(lin_poly_rss[0], lin_poly_rss[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
